{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5c87234",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np \n",
    "import requests\n",
    "import os\n",
    "import datetime as dt\n",
    "import statsmodels.api as sm\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from scipy.optimize import minimize\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#symbols = ('AAPL','MSFT','AMZN','NVDA','GOOGL','BRK.B','GOOG','TSLA','META','XOM')\n",
    "\n",
    "factors = pd.read_csv('F-F_Research_Data_5_Factors_2x3_daily.CSV',index_col=0)/100\n",
    "factors.index = factors.index.astype(int)\n",
    "factors.index = pd.to_datetime(factors.index, format='%Y%m%d')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9667e046",
   "metadata": {},
   "outputs": [],
   "source": [
    "symbols = ('AAPL','MSFT','AMZN','NVDA','GOOGL','BRK.B','GOOG','TSLA','META','XOM','UNH','JNJ','JPM',\n",
    "           'V','PG','MA','HD','CVX','ABBV','LLY','MRK','AVGO','PEP','KO','PFE','TMO','COST','CSCO',\n",
    "           'WMT','MCD','BAC','CRM','DIS','ACN','ADBE','LIN','ABT','TXN','DHR','VZ','AMD','CMCSA',\n",
    "           'NEE','NFLX','PM','NKE','BMY','RTX','WFC','QCOM','UPS','ORCL','T','INTC','AMGN','HON',\n",
    "           'INTU','COP','UNP','BA','CAT','IBM','LOW','SBUX','MS','PLD','SPGI','ELV','DE','GS','LMT',\n",
    "           'MDT','AMAT','GE','GILD','BKNG','BLK','ADI','AXP','CVS','MDLZ','SYK','AMT','ADP','NOW',\n",
    "           'TJX','C','ISRG','REGN','TMUS','PYPL','PGR','MMC','VRTX','CB','SCHW','MO','ZTS','CI','SO',\n",
    "           'DUK','TGT','LRCX','FISV','BSX','BDX','SLB','MU','CME','ETN','ITW','NOC','EOG','EQIX','AON',\n",
    "           'CL','APD','CSX','HUM','MPC','ATVI','WM','FCX','ICE','SNPS','CCI','EL','CDNS','MMM','HCA',\n",
    "           'KLAC','VLO','ORLY','FDX','SHW','PNC','GD','EW','GIS','GM','USB','MCK','EMR','MRNA','F','PXD',\n",
    "           'APH','NSC','MCO','MSI','NXPI','SRE','PSX','PSA','DG','D','AEP','ROP','CMG','MCHP','OXY','AZO',\n",
    "           'KMB','TFC','DXCM','MSCI','MAR','ADM','ADSK','PH','CTVA','FTNT','TT','EXC','TEL','IDXX','JCI',\n",
    "           'ECL','CTAS','AJG','ANET','MNST','A','BIIB','TRV','O','SYY','NEM','NUE','TDG','DOW','CARR',\n",
    "           'PCAR','MET','HSY','LHX','AIG','CHTR','HES','HLT','XEL','STZ','YUM','PAYX','AFL','IQV','COF',\n",
    "           'ROST','WMB','ILMN','CNC','ON','SPG','OTIS','KMI','ED','WELL','MTD','CMI','ROK','BK','WBD',\n",
    "           'AME','DVN','DD','AMP','VICI','KR','RMD','CPRT','EA','FIS','PEG','CTSH','KHC','PPG','ODFL',\n",
    "           'FAST','GWW','PRU','VRSK','GEHC','DHI','WEC','APTV','KDP','DLTR','ALL','BKR','ANSS','KEYS',\n",
    "           'HAL','AWK','OKE','ENPH','SBAC','CSGP','RSG','ULTA','ES','GPN','DLR','URI','EIX','ZBH','GLW',\n",
    "           'DFS','STT','ALB','LEN','ABC','CDW','PCG','TSCO','WST','IT','CEG','ACGL','HPQ','WBA','TROW',\n",
    "           'WTW','FANG','EFX','PWR','EBAY','FTV','LYB','IR','GPC','IFF','AVB','ALGN','VMC','CBRE','AEE',\n",
    "           'MPWR','PODD','HIG','DAL','ETR','MLM','WY','FE','EXR','CHD','FSLR','DTE','DOV','TDY','BAX',\n",
    "           'MKC','MTB','HPE','PPL','HOLX','EQR','LH','CAH','ARE','VRSN','CLX','CTRA','OMC','LUV','DRI',\n",
    "           'SWKS','STE','TTWO','CNP','XYL','STLD','NDAQ','LVS','WAT','COO','NTRS','FITB','WAB','CMS',\n",
    "           'VTR','RJF','INVH','CAG','FICO','CINF','RF','MAA','BR','K','EXPD','IEX','BALL','TSN','PFG',\n",
    "           'SJM','TER','EPAM','AMCR','PKI','SEDG','NVR','TRGP','ATO','ZBRA','HBAN','DGX','AES','FDS',\n",
    "           'HWM','MOH','MOS','FMC','GRMN','FLT','BBY','CFG','IRM','MRO','J','TXT','LW','MKTX','PAYC',\n",
    "           'BG','TYL','EXPE','UAL','AVY','IPG','RCL','LKQ','CF','JBHT','RE','CBOE','EVRG','MGM','ETSY',\n",
    "           'NTAP','PTC','BRO','LNT','INCY','ESS','PHM','POOL','PKG','SNA','TRMB','IP','STX','WRB','SYF',\n",
    "           'LDOS','UDR','AKAM','WDC','PEAK','NDSN','DPZ','TFX','CTLT','VTRS','KEY','APA','KIM','BWA','BF.B','SWK','TECH','WYNN','CPT','EQT','CHRW','HRL','HST','NI','L','CPB','JNPR','PARA','MAS','HSIC','JKHY','CDAY','CE','MTCH','QRVO','FOXA','CCL','TPR','CRL','BIO','EMN','CZR','GL','KMX','LYV','TAP','GEN','AAL','ALLE','REG','PNW','PNR','AOS','ROL','RHI','FFIV','XRAY','BBWI','HII','UHS','NRG','WRK','BEN','BXP','VFC','IVZ','AAP','WHR','FRT','GNRC','HAS','NWSA','SEE','AIZ','OGN','CMA','DXC','NCLH','ALK','MHK','RL','ZION','NWL','DVA','FOX','LNC','FRC','DISH','NWS'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d624bc0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import yfinance as yf\n",
    "\n",
    "#symbols = ['AAPL', 'TSLA', 'MSFT']\n",
    "\n",
    "# Download stock data for the symbols\n",
    "data = yf.download(symbols, start='2012-01-01', end='2022-01-01')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b15052ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Cleaning and Retrun Calculation\n",
    "#Spliting Data into Train-test:\n",
    "Close = data['Adj Close']\n",
    "volume = data['Volume']\n",
    "returns = Close.pct_change()[1:]\n",
    "df = returns.join(factors,how='inner')\n",
    "rfacts = df.iloc[:,-6:-1]\n",
    "rRets = df.iloc[:,:-6]\n",
    "rRF = df.iloc[:,-1:]\n",
    "exess_ret = rRets.subtract(rRF,fill_value=0)\n",
    "exess_ret.fillna(0,inplace=True)\n",
    "\n",
    "\n",
    "\n",
    "# Split data into training and testing sets\n",
    "def Train_test_split(data_asset,data_factors,cuttoff):\n",
    "    train_asset_returns = data_asset.loc[data_asset.index <= pd.to_datetime(cuttoff)]\n",
    "    train_factor_returns = rfacts.loc[data_factors.index <= pd.to_datetime(cuttoff)]\n",
    "\n",
    "    test_asset_returns = data_asset.loc[data_asset.index >= pd.to_datetime(cuttoff)]\n",
    "    test_factor_returns = data_factors.loc[data_factors.index >= pd.to_datetime(cuttoff)]\n",
    "    \n",
    "    return train_asset_returns, train_factor_returns, test_asset_returns, test_factor_returns\n",
    "\n",
    "\n",
    "cutoff_Date ='2021-01-01'\n",
    "\n",
    "train_stock_returns = Train_test_split(exess_ret,rfacts,cuttoff=cutoff_Date)[0]\n",
    "train_factors = Train_test_split(exess_ret,rfacts,cuttoff=cutoff_Date)[1]\n",
    "\n",
    "test_stock_returns = Train_test_split(exess_ret,rfacts,cuttoff=cutoff_Date)[2]\n",
    "test_factors = Train_test_split(exess_ret,rfacts,cuttoff=cutoff_Date)[3]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58043c39",
   "metadata": {},
   "source": [
    "# Standardizing Factors\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e19de82b",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "normalized_factors = scaler.fit_transform(train_factors)\n",
    "normalized_factors = pd.DataFrame(normalized_factors, index=train_factors.index, columns=train_factors.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9166ba8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import statsmodels.api as sm\n",
    "\n",
    "class PortfolioOptimizer:\n",
    "    def __init__(self, dependent_data, independent_data, window_size, dependent_vars, independent_vars, decay_rate, prediction_step=1):\n",
    "        self.dependent_data = dependent_data\n",
    "        self.independent_data = independent_data\n",
    "        self.window_size = window_size\n",
    "        self.dependent_vars = dependent_vars\n",
    "        self.independent_vars = independent_vars\n",
    "        self.decay_rate = decay_rate\n",
    "        self.prediction_step = prediction_step\n",
    "        self.all_factor_loadings, self.all_predictions = self.rolling_multiple_regression_with_prediction_multi_assets()\n",
    "\n",
    "    def rolling_multiple_regression_with_prediction_multi_assets(self):\n",
    "        all_factor_loadings = {}\n",
    "        all_predictions = {}\n",
    "\n",
    "        for dependent_var in self.dependent_vars:\n",
    "            factor_loadings, predictions = self.rolling_multiple_regression_with_prediction(self.dependent_data, self.independent_data, self.window_size, dependent_var, self.independent_vars, self.decay_rate, self.prediction_step)\n",
    "\n",
    "            all_factor_loadings[dependent_var] = factor_loadings\n",
    "            all_predictions[dependent_var] = predictions\n",
    "\n",
    "        return all_factor_loadings, all_predictions\n",
    "\n",
    "    def rolling_multiple_regression_with_prediction(self, dependent_data, independent_data, window_size, dependent_var, independent_vars, decay_rate, prediction_step=1):\n",
    "        factor_loadings = []\n",
    "        predictions = []\n",
    "\n",
    "        for i in range(len(dependent_data) - window_size - prediction_step + 1):\n",
    "            # Slice the data for the current window\n",
    "            window_dependent_data = dependent_data.iloc[i:i + window_size]\n",
    "            window_independent_data = independent_data.iloc[i:i + window_size]\n",
    "\n",
    "            # Apply exponential decay weights to emphasize recent data\n",
    "            weights = np.exp(np.arange(window_size) * -decay_rate)\n",
    "            weights = weights[::-1]  # Reverse the order of weights to put more emphasis on recent data\n",
    "\n",
    "            # Fit the multiple regression model\n",
    "            X_train = window_independent_data[independent_vars]\n",
    "            X_train = sm.add_constant(X_train)  # Add a constant for the intercept term\n",
    "            y_train = window_dependent_data[dependent_var]\n",
    "\n",
    "            model = sm.WLS(y_train, X_train, weights=weights).fit()\n",
    "\n",
    "            # Store factor loadings (coefficients of independent variables)\n",
    "            factor_loadings.append(model.params[1:].values)\n",
    "\n",
    "            # Make out-of-sample predictions\n",
    "            X_test = sm.add_constant(independent_data).iloc[i + window_size:i + window_size + prediction_step]\n",
    "            prediction = model.predict(X_test)\n",
    "            predictions.append(prediction.iloc[0])  # Extract the first element of the prediction series\n",
    "\n",
    "        # Choose the last value factor loadings as the most accurate regression model\n",
    "        last_factor_loadings = {}\n",
    "        for dependent_var in self.dependent_vars:\n",
    "            last_factor_loadings[dependent_var] = factor_loadings[-1]\n",
    "\n",
    "        loadings = pd.DataFrame(last_factor_loadings, index=independent_vars)\n",
    "\n",
    "        return loadings, predictions\n",
    "\n",
    "    def portfolio_risk_calculation(self, train_stock_returns, train_factors):\n",
    "        last_factor_loadings = {}\n",
    "\n",
    "        # Step 1: Get the latest factor loadings for each dependent variable\n",
    "        for dependent_var in self.dependent_vars:\n",
    "            last_factor_loadings[dependent_var] = self.all_factor_loadings[dependent_var][-1]\n",
    "\n",
    "        loadings = pd.DataFrame(last_factor_loadings, index=train_factors.columns)\n",
    "\n",
    "        # Step 2: Estimate factor returns using cross-sectional regression for each date\n",
    "        factor_returns = []\n",
    "        for date in train_stock_returns.index:\n",
    "            X = sm.add_constant(loadings.T)    # Factor exposures for the current date\n",
    "            y = train_stock_returns.loc[date]  # Stock returns for the current date\n",
    "\n",
    "            model = sm.OLS(y, X).fit()\n",
    "            factor_returns.append(model.params[1:].values)\n",
    "\n",
    "        factor_returns = pd.DataFrame(factor_returns, index=train_factors.index, columns=train_factors.columns)\n",
    "\n",
    "        # Step 3: Calculate residual risk\n",
    "        residual_risk = []\n",
    "\n",
    "        for date in train_stock_returns.index:\n",
    "            X = sm.add_constant(loadings.T)\n",
    "            y = train_stock_returns.loc[date]\n",
    "\n",
    "            model = sm.OLS(y, X).fit()\n",
    "            residuals = model.resid\n",
    "            residual_risk.append(residuals)\n",
    "\n",
    "        residual_risk = pd.DataFrame(residual_risk, index=train_stock_returns.index)\n",
    "\n",
    "        F_COV = factor_returns.cov()\n",
    "        DELTA_COV = residual_risk.cov()\n",
    "\n",
    "        # Constrain the DELTA_COV matrix to be positive semi-definite\n",
    "        eigenvalues, eigenvectors = np.linalg.eigh(DELTA_COV)\n",
    "        eigenvalues = np.maximum(eigenvalues, 0)\n",
    "        DELTA_COV = eigenvectors @ np.diag(eigenvalues) @ eigenvectors.T\n",
    "\n",
    "        weights_box = np.full(shape=(len(residual_risk.columns), 1), fill_value=1 / len(residual_risk.columns))\n",
    "        FactorloadingMatrix = loadings.T\n",
    "\n",
    "        # Common Variance\n",
    "        Common_Variance = np.linalg.multi_dot([weights_box.T, FactorloadingMatrix, F_COV, FactorloadingMatrix.T, weights_box])[0][0]\n",
    "        Common_STD = np.sqrt(Common_Variance) * np.sqrt(252)\n",
    "\n",
    "        # Specific Variance\n",
    "        Specific_return_Volatility = np.sqrt(np.linalg.multi_dot([weights_box.T, DELTA_COV, weights_box]))[0][0]\n",
    "\n",
    "        return Common_STD, Specific_return_Volatility\n",
    "    \n",
    "    def portfolio2_objective(Weights, factor_returns, FactorloadingMatrix, residual_risk, risk_aversion):\n",
    "    product = np.dot(factor_returns, FactorloadingMatrix.T)\n",
    "    total_expected_returns = np.dot(product, Weights).sum(axis=0) + residual_risk.to_numpy().flatten()\n",
    "    portfolio_returns = total_expected_returns.mean()\n",
    "    portfolio_variance = np.linalg.multi_dot([Weights.T, DELTA_COV, Weights]) +\\\n",
    "    np.linalg.multi_dot([Weights.T, FactorloadingMatrix, F_COV, FactorloadingMatrix.T, Weights])\n",
    "    return -1 * (portfolio_returns/ np.sqrt(portfolio_variance))\n",
    "\n",
    "\n",
    "    # Set risk aversion parameter\n",
    "\n",
    "\n",
    "    def common_Risk(Weights):\n",
    "        var = np.linalg.multi_dot([Weights.T,FactorloadingMatrix,HL_COV,\n",
    "                             FactorloadingMatrix.T,Weights]) \n",
    "        return  var\n",
    "\n",
    "    def Specific_risk(Weights):\n",
    "        #s_vec = np.diag(HL_DELTA) * cross_reg_resid.shape[0] / (cross_reg_resid.shape[0]-1)\n",
    "        s_var = np.linalg.multi_dot([Weights.T,HL_DELTA,Weights])\n",
    "        #s_var = s_var \n",
    "        return  s_var\n",
    "\n",
    "    def Tota_risk(Weights):\n",
    "        var = np.linalg.multi_dot([Weights.T,FactorloadingMatrix,F_COV,\n",
    "                             FactorloadingMatrix.T,Weights])\n",
    "        #s_vec = np.diag(HL_DELTA) * cross_reg_resid.shape[0] / (cross_reg_resid.shape[0]-1)\n",
    "        s_var = np.linalg.multi_dot([Weights.T,DELTA_COV,Weights])\n",
    "        return var + s_var\n",
    "\n",
    "    #def weighted_betas(Weights):\n",
    "       # XP = np.dot(Weights.T,FactorloadingMatrix)\n",
    "        #return XP\n",
    "    factor_list = np.arange(len(FactorloadingMatrix.columns))\n",
    "    varindx=3\n",
    "\n",
    "    def check_sum(Weights):\n",
    "        return np.sum(Weights) - 1 \n",
    "\n",
    "    def Stock_drop(Weights):\n",
    "        return Weights[[0,5]] - [0.05,0.2]\n",
    "\n",
    "    def Factor_bound(Weights):\n",
    "        XP = np.dot(Weights.T,FactorloadingMatrix)\n",
    "        return XP[factor_list[varindx]] + 1.5\n",
    "\n",
    "    def portfolio_objective_with_alphas(Weights, factor_returns, FactorloadingMatrix, residual_risk, risk_aversion, alphas):\n",
    "        product = np.dot(factor_returns, FactorloadingMatrix.T)\n",
    "        total_expected_returns = np.dot(product, Weights).sum(axis=0) + residual_risk.to_numpy().flatten() + alphas\n",
    "        portfolio_returns = total_expected_returns.mean()\n",
    "        portfolio_variance = np.linalg.multi_dot([Weights.T, DELTA_COV, Weights]) +\\\n",
    "        np.linalg.multi_dot([Weights.T, FactorloadingMatrix, F_COV, FactorloadingMatrix.T, Weights])\n",
    "        return -1 * (portfolio_returns - 0.5 * risk_aversion * portfolio_variance)\n",
    "    \n",
    "\n",
    "\n",
    "    def generate_alphas(num_stocks):\n",
    "        # Replace this with your sentiment analysis implementation\n",
    "        alphas = np.random.uniform(-0.01, 0.01, num_stocks)\n",
    "        return alphas\n",
    "\n",
    "\n",
    "    def generate_sentiment_scores(text_data, tokenizer, model, device):\n",
    "        model.eval()\n",
    "        sentiment_scores = []\n",
    "\n",
    "        for text in text_data:\n",
    "            inputs = tokenizer(text, return_tensors=\"pt\", padding=True, truncation=True).to(device)\n",
    "            output = model(**inputs)\n",
    "            sentiment_score = output.logits.detach().cpu().numpy().flatten()\n",
    "            sentiment_scores.append(sentiment_score)\n",
    "\n",
    "        return np.array(sentiment_scores)\n",
    "\n",
    "\n",
    "    def generate_alphas(sentiment_scores):\n",
    "        alphas = sentiment_scores  # You can apply any transformation to sentiment_scores if needed.\n",
    "        return alphas\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    def omega_ratio(returns, threshold=0):\n",
    "        returns = np.array(returns)\n",
    "        gain_returns = returns[returns > threshold]\n",
    "        loss_returns = returns[returns <= threshold]\n",
    "\n",
    "        gain_returns_mean = gain_returns.mean() if len(gain_returns) > 0 else 0\n",
    "        loss_returns_mean = -loss_returns.mean() if len(loss_returns) > 0 else 0\n",
    "\n",
    "        return gain_returns_mean / loss_returns_mean\n",
    "\n",
    "    def sortino_ratio(returns, threshold=0):\n",
    "        returns = np.array(returns)\n",
    "        excess_returns = returns - threshold\n",
    "        downside_returns = excess_returns[excess_returns < 0]\n",
    "\n",
    "        downside_volatility = np.std(downside_returns) * np.sqrt(252)\n",
    "        annualized_return = np.mean(returns) * 252\n",
    "\n",
    "        return annualized_return / downside_volatility\n",
    "\n",
    "    def max_drawdown(returns):\n",
    "        cumulative_returns = np.cumprod(1 + returns)\n",
    "        max_return = cumulative_returns.cummax()\n",
    "        drawdowns = (cumulative_returns - max_return) / max_return\n",
    "        return drawdowns.min()\n",
    "\n",
    "    def tracking_error(portfolio_returns, benchmark_returns):\n",
    "        return_difference = np.array(portfolio_returns) - np.array(benchmark_returns)\n",
    "        return np.std(return_difference) * np.sqrt(252)\n",
    "\n",
    "    def information_ratio(portfolio_returns, benchmark_returns):\n",
    "        return_difference = np.array(portfolio_returns) - np.array(benchmark_returns)\n",
    "        return_difference_mean = np.mean(return_difference)\n",
    "        tracking_err = tracking_error(portfolio_returns, benchmark_returns)\n",
    "\n",
    "        return return_difference_mean / tracking_err\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0634f405",
   "metadata": {},
   "source": [
    "# Inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbc83110",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate factor loadings using rolling_multiple_regression_with_prediction_multi_assets\n",
    "window = 2263\n",
    "decay = 0.1\n",
    "steps = 1 #prediction_step\n",
    "dependent_vars = train_stock_returns.columns\n",
    "independent_vars = train_factors.columns\n",
    "risk_aversion = 0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a4bb510",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the PortfolioOptimizer class\n",
    "optimizer = PortfolioOptimizer(dependent_data, independent_data, window_size, dependent_vars, independent_vars, decay_rate, prediction_step=1)\n",
    "\n",
    "# Print input\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e750f8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Portfolio optimization\n",
    "num_assets = len(train_stock_returns.columns)\n",
    "initial_weights = np.full((1,num_assets),1/num_assets)[0].reshape(num_assets,1)\n",
    "bounds = [(0, 1) for _ in range(num_assets)]\n",
    "# bounds = tuple(bound for asset in range(num_assets))\n",
    "constraints = [{'type': 'eq', 'fun': lambda w: np.sum(w) - 1}]\n",
    "# cons = ({'type':'eq', 'fun':check_sum})\n",
    "#        # {'type':'eq','fun':Stock_drop},\n",
    "#        #,{'type':'eq','fun':Factor_bound})\n",
    "\n",
    "variables = [common_Risk, Tota_risk, Specific_risk,portfolio_objective,portfolio2_objective]\n",
    "\n",
    "# optimized_weights = minimize(variables[2], initial_weights.ravel(), args=(factor_returns,\\\n",
    "#                                                                          FactorloadingMatrix,\\\n",
    "#                                                                          residual_risk, risk_aversion),\\\n",
    "#                              bounds=bounds, constraints=constraints)\n",
    "\n",
    "# optimized_weights = minimize(variables[3], initial_weights.ravel(), args=(factor_returns, \n",
    "#                                weights, risk_cap, factor_loadings, factor_cov_matrix, \n",
    "#                                risk_aversion), method='SLSQP', constraints=constraints)\n",
    "\n",
    "optimized_weights_with_alpha = minimize(alpha_portfolio_objective, initial_weights.ravel(), args=(factor_returns,\\\n",
    "                         FactorloadingMatrix, residual_risk, alphas, risk_aversion), method='SLSQP', bounds=bounds, constraints=constraints)\n",
    "\n",
    "\n",
    "optimized_weights_with_alphas = minimize(portfolio_objective_with_alphas, initial_weights.ravel(), args=(factor_returns,\\\n",
    "                                 FactorloadingMatrix,\\\n",
    "                                 residual_risk, risk_aversion, alphas),\\\n",
    "                                 bounds=bounds, constraints=constraints)\n",
    "\n",
    "#optimized_weights_with_alphas = minimize(portfolio_objective_with_alphas, initial_weights.ravel(), args=(factor_returns,\\\n",
    "#                                  FactorloadingMatrix,\\\n",
    "#                                  residual_risk, risk_aversion, alphas),\\\n",
    "#                                  bounds=bounds, constraints=constraints)\n",
    "\n",
    "\n",
    "out_of_sample_returns_with_alpha = np.dot(test_stock_returns, optimized_weights_with_alpha.x)\n",
    "\n",
    "\n",
    "cumulative_returns_with_alpha = np.cumprod(1 + out_of_sample_returns_with_alpha) - 1\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "optimized_weights = minimize(variables[1],initial_weights.ravel(),method='SLSQP',bounds=bounds,constraints=constraints)\n",
    "\n",
    "#print(\"Optimized weights:\", optimized_weights.x)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
